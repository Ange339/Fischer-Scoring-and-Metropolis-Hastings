{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(random_state=42, n_samples=10000, n_features=10, n_redundant=0) # generate a random dataset\n",
    "y = y.reshape(-1,1) # make Y a column vector\n",
    "\n",
    "# add a column of ones to X\n",
    "ones = np.ones((X.shape[0],1))\n",
    "X = np.hstack((ones,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fischer Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convergence reached with value: 1.0491831950805439e-07\n",
      "Number of iteration: 9\n",
      "Time taken:  48.62 seconds\n",
      "\n",
      "Parameters: [-6.66455910e-02  2.31702932e-03 -1.97892969e-02  4.08063752e-01\n",
      "  3.69850871e-02 -2.35247404e-02 -1.74722354e-02  1.47049619e+00\n",
      " -6.92819888e-03 -7.36593907e-04  4.58366051e-03]\n"
     ]
    }
   ],
   "source": [
    "def probit(X, y, epsilon, verbose = True):\n",
    "    n, p = np.shape(X)\n",
    "    \n",
    "    b_0 = np.zeros((p,1)) # Setting initial value of beta as 0\n",
    "    Xb = np.dot(X,b_0) # Calculating Xb\n",
    "    mu = norm.cdf(Xb) # Calculating mu\n",
    "    W = np.diag(((norm.pdf(Xb)**2)/ (mu*(1-mu))).flatten()) # Calculating W\n",
    "    Z = Xb + (y - mu)/norm.pdf(Xb) # Calculating Z\n",
    "\n",
    "    iteration = 0 # Set the number of iterations\n",
    "\n",
    "    start_time = time.process_time() # Start the timer\n",
    "    while True: ### Algorithm Starts\n",
    "        iteration += 1\n",
    "\n",
    "        # Iterative updates\n",
    "        b = np.linalg.inv(X.T @ W @ X) @ X.T @ W @ Z\n",
    "        Xb = np.dot(X,b)\n",
    "        mu = norm.cdf(Xb)\n",
    "        W = np.diag(((norm.pdf(Xb)**2)/(mu*(1-mu))).flatten())\n",
    "        Z = Xb + (y - mu)/norm.pdf(Xb)\n",
    "\n",
    "        # Calculating the convergence criteria\n",
    "        delta = np.linalg.norm(b-b_0)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iteration:, {iteration}, Converging = {delta}\")\n",
    "\n",
    "        if delta < epsilon: # Checking for convergence\n",
    "            print(\"\\nConvergence reached with value:\", delta)\n",
    "            print(\"Number of iteration:\", iteration)\n",
    "            print(f\"Time taken: {time.process_time() - start_time : .2f} seconds\")\n",
    "            break\n",
    "\n",
    "        b_0 = b \n",
    "    \n",
    "    print(f\"\\nParameters: {b.flatten()}\")\n",
    "    return(b.flatten())\n",
    "\n",
    "b = probit(X, y, epsilon, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metropolis Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "def probit_link_function(X, beta):\n",
    "    linear_pred = np.dot(X, beta)\n",
    "    return stats.norm.cdf(linear_pred)\n",
    "def log_likelihood(y, X, beta):\n",
    "    p = probit_link_function(X, beta)\n",
    "    p = np.clip(p, EPSILON, 1 - EPSILON)\n",
    "    log_lik = np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "    return log_lik\n",
    "def log_prior(beta):\n",
    "    prior = stats.norm.logpdf(beta, loc=0, scale=1)\n",
    "    return np.sum(prior)\n",
    "\n",
    "def metropolis_hastings(y, X, num_iterations, initial_beta, proposal_sd):\n",
    "    beta = initial_beta\n",
    "    accepted_samples = [beta]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        current_log_lik = log_likelihood(y, X, beta)\n",
    "        current_log_prior = log_prior(beta)\n",
    "        current_log_posterior = current_log_lik + current_log_prior\n",
    "\n",
    "        proposed_beta = beta + np.random.normal(0, proposal_sd, size=len(beta))\n",
    "        \n",
    "        proposed_log_lik = log_likelihood(y, X, proposed_beta)\n",
    "        proposed_log_prior = log_prior(proposed_beta)\n",
    "        proposed_log_posterior = proposed_log_lik + proposed_log_prior\n",
    "        \n",
    "        acceptance_ratio = min(1, np.exp(proposed_log_posterior - current_log_posterior))\n",
    "        \n",
    "        if acceptance_ratio > np.random.rand():\n",
    "            beta = proposed_beta\n",
    "            accepted_samples.append(beta)\n",
    "        else:\n",
    "            accepted_samples.append(beta)\n",
    "    return np.array(accepted_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
